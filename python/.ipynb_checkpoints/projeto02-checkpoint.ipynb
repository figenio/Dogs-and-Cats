{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ass Mateus e Yan\n",
    "\n",
    "# IMPORTAÇÕES\n",
    "import numpy as np\n",
    "import os, cv2, random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIÇÕES\n",
    "\n",
    "\n",
    "TRAIN_DIR = 'kaggle/train/'\n",
    "\n",
    "num_img = 1000                      # Opção do número de imagens a serem usadas\n",
    "n_classes = 2                     # A base de dados DogsAndCats tem 2 classes de objetos!!!!\n",
    "nepochs = 10                      # Numero de epocas para o treinamento!!!\n",
    "batch_size = 32                   # Numero de imagens por batch!!!\n",
    "image_size = 128                   # Todas as imagens devem ser redimensionadas para 32x32 pixels!!!\n",
    "nchannels = 3                     # Numero de canais de cores na imagem!!!\n",
    "n_input = image_size * image_size * nchannels # Tamanho da entrada!\n",
    "learning_rate = 1e-3              # Taxa de aprendizado!!!\n",
    "kprob = 0.5                       # Probabilidade para dropout!!!\n",
    "test_size = 0.25                  # Porcentagem que sobra pra teste\n",
    "random_state = 42                 # Semente de aleatoriedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÕES AUXILIARES\n",
    "\n",
    "\n",
    "# Retorna a imagem apontada pelo endereço do arquivo redimensionada para o tamnho correto\n",
    "# Temos a opção de ler a imagem colorida ou em escala de cinza\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "#    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return cv2.resize(img, (image_size, image_size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "# Preparação dos Dados\n",
    "# Normaliza e passa as imagens no formato de um vetor de dados de tipo dtype\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, nchannels, image_size, image_size), dtype = np.float32)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        \n",
    "        #primeira forma de normalização\n",
    "        #min_val = np.min(image)\n",
    "        #max_val = np.max(image )\n",
    "        #image = (image-min_val)/(max_val-min_val)\n",
    "        #segunda forma de normalização\n",
    "        image = image/255.0    \n",
    "        \n",
    "        data[i] = image.T\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))    \n",
    "    return data\n",
    "\n",
    "        \n",
    "#---------------------------------------------\n",
    "\n",
    "def create_label(image_name):\n",
    "    word_label = image_name.split('.')[-3]\n",
    "    if word_label == 'cat':\n",
    "        return np.array([1,0])\n",
    "    elif word_label == 'dog':\n",
    "        return np.array([0,1])\n",
    "    else: \n",
    "        print (\"Esta classe não existe!!!!!\")\n",
    "\n",
    "\n",
    "def read_dataset (filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for img in tqdm(os.listdir(filename)):\n",
    "        path = os.path.join(filename, img)\n",
    "        img_data = cv2.imread(path)\n",
    "        #img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE) #converte para níveis de cinza\n",
    "        img_data = cv2.resize(img_data, (image_size, image_size)) #deixa com as dimensões definidas\n",
    "        \n",
    "        #primeira forma de normalização\n",
    "        #min_val = np.min(img_data)\n",
    "        #max_val = np.max(img_data )\n",
    "        #img_data = (img_data-min_val)/(max_val-min_val)\n",
    "        #segunda forma de normalização\n",
    "        img_data = img_data/255.0    \n",
    "        \n",
    "        #cria os vetores de dados e de labels\n",
    "        X.append(np.array(img_data))\n",
    "        Y.append(np.array(create_label(img)))\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "\n",
    "def next_batch (num, data, labels_n):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = labels_n[idx]\n",
    "    return data_shuffle, labels_shuffle\n",
    "\n",
    "\n",
    "# Printa a imagem de um gato e de um cachorro no indice apontado\n",
    "def show_cats_and_dogs(idx):\n",
    "    cat = read_image(train_cats[idx])\n",
    "    dog = read_image(train_dogs[idx])\n",
    "    pair = np.concatenate((cat, dog), axis=1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(pair)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dois exemplos de descritores dos professores\n",
    "\n",
    "# Nesse a imagem é redimensionada e achatada a imagem num vetor de características\n",
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "    return cv2.resize(image, size).flatten()\n",
    "\n",
    "# Neste segundo é extraído um histograma do HSV e é achatado\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n",
    "        [0, 180, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOSSOS DESCRITORES\n",
    "\n",
    "#CNN igual ao do slide\n",
    "def cnn (x, prob):\n",
    "    \n",
    "    input_layer = tf.reshape(x, shape = [-1, image_size, image_size, nchannels])\n",
    "    \n",
    "    # Primeira camada de convolucao:\n",
    "    conv1 = tf.layers.conv2d (\n",
    "                inputs=input_layer,\n",
    "                filters=32,\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu\n",
    "            )\n",
    "    \n",
    "    # Primeira camada de pooling:\n",
    "    pool1 = tf.layers.max_pooling2d (\n",
    "                inputs=conv1, \n",
    "                pool_size=[2, 2], \n",
    "                strides=2\n",
    "            )\n",
    "    \n",
    "    # Segunda camada de convolucao:\n",
    "    conv2 = tf.layers.conv2d (\n",
    "                inputs=pool1,\n",
    "                filters=64,\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu\n",
    "            )\n",
    "    \n",
    "    # Segunda camada de pooling:\n",
    "    pool2 = tf.layers.max_pooling2d (\n",
    "                inputs=conv2, \n",
    "                pool_size=[2, 2], \n",
    "                strides=2\n",
    "            )\n",
    "    \n",
    "    flat = tf.contrib.layers.flatten(pool2)\n",
    "    \n",
    "    # Aqui é descomentado quando se quer retornar apenas o descritor\n",
    "#    return flat\n",
    "    \n",
    "    dense = tf.contrib.layers.fully_connected(\n",
    "                inputs = flat,\n",
    "                num_outputs = 128,\n",
    "                activation_fn = tf.nn.relu\n",
    "            )\n",
    "    \n",
    "    dropout = tf.nn.dropout(dense, prob)\n",
    "    \n",
    "    out = tf.contrib.layers.fully_connected(\n",
    "                inputs = dropout,\n",
    "                num_outputs = n_classes,\n",
    "                activation_fn = None\n",
    "            )\n",
    "    \n",
    "    # Aqui retornamos o classificador em cnn\n",
    "    return out\n",
    "\n",
    "\n",
    "# MLP da lista de exercícios\n",
    "def mlp (x):\n",
    "    # Aqui é definido o número de neurônios em cada camada\n",
    "    n_camada_1 = 256\n",
    "    n_camada_2 = 128\n",
    "   \n",
    "    # Primeira camada da rede:\n",
    "    W1 = tf.get_variable('w1', [n_input, n_camada_1], initializer = tf.random_normal_initializer())\n",
    "    b1 = tf.get_variable('b1', [n_camada_1], initializer = tf.random_normal_initializer())\n",
    "    y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1) #tf.nn.relu() or tf.matmul(x, W1) + b1!!!! \n",
    "\n",
    "    # Segunda camada da rede:\n",
    "    W2 = tf.get_variable('w2', [n_camada_1, n_camada_2], initializer = tf.random_normal_initializer())\n",
    "    b2 = tf.get_variable('b2', [n_camada_2], initializer = tf.random_normal_initializer())\n",
    "    y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2) \n",
    "\n",
    "    # Ultima camada da rede:\n",
    "    W3 = tf.get_variable('w3',[n_camada_2, n_classes], initializer = tf.random_normal_initializer())\n",
    "    b3 = tf.get_variable('b3',[n_classes], initializer = tf.random_normal_initializer())\n",
    "    out_layer = tf.matmul(y2, W3) + b3 \n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 2000\n",
      "Processed 250 of 2000\n",
      "Processed 500 of 2000\n",
      "Processed 750 of 2000\n",
      "Processed 1000 of 2000\n",
      "Processed 1250 of 2000\n",
      "Processed 1500 of 2000\n",
      "Processed 1750 of 2000\n",
      "Train shape: (2000, 3, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "#MONTANDO A BASE DE TREINAMENTO\n",
    "\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] #full dataset: dogs and cats\n",
    "train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "\n",
    "# considera apenas num_img imagens. Para o dataset completo, desconsiderar.\n",
    "train_images = train_dogs[:num_img] + train_cats[:num_img]\n",
    "random.shuffle(train_images)\n",
    "\n",
    "# Leitura das imagens\n",
    "train = prep_data(train_images)\n",
    "print(\"Train shape: {}\".format(train.shape))\n",
    "\n",
    "# Cria os labels (rótulos)\n",
    "labels = []\n",
    "labels_n = []\n",
    "for i in train_images:\n",
    "    if 'dog' in i:\n",
    "        labels.append(1)\n",
    "        labels_n.append(np.array([0,1]))\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        labels_n.append(np.array([1,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apenas mostra algumas imagens do conjunto de treinamento\n",
    "\n",
    "\n",
    "#for idx in range(0,3):\n",
    " #   show_cats_and_dogs(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 2000\n",
      "Processed 250 of 2000\n",
      "Processed 500 of 2000\n",
      "Processed 750 of 2000\n",
      "Processed 1000 of 2000\n",
      "Processed 1250 of 2000\n",
      "Processed 1500 of 2000\n",
      "Processed 1750 of 2000\n"
     ]
    }
   ],
   "source": [
    "#Aqui passa cada imagem pelos descritores e salva nos vetores abaixo\n",
    "\n",
    "rawImages = []\n",
    "descHist = []\n",
    "\n",
    "count = len(train_images)\n",
    "\n",
    "for i, image_file in enumerate(train_images):\n",
    "    image = read_image(image_file)\n",
    "    pixels = image_to_feature_vector(image)\n",
    "    histogram = extract_color_histogram(image)\n",
    "    \n",
    "    rawImages.append(pixels)\n",
    "    descHist.append(histogram)\n",
    "        \n",
    "    if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "accuracy: 57.80%\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "accuracy: 55.60%\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "accuracy: 54.40%\n"
     ]
    }
   ],
   "source": [
    "#Avalia o primeiro descritor: as imagens raw\n",
    "#Usa KNN, Arvore e Gaussian\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(\n",
    "    rawImages, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(17),    \n",
    "    DecisionTreeClassifier(),\n",
    "    GaussianNB()]\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(\"accuracy: {:.2f}%\".format(acc * 100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "accuracy: 60.20%\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "accuracy: 61.80%\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "accuracy: 57.60%\n"
     ]
    }
   ],
   "source": [
    "#Avalia o segundo descritor: color histogram\n",
    "#Novamente com os três classificadores\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(\n",
    "    descHist, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(17),    \n",
    "    DecisionTreeClassifier(),\n",
    "    GaussianNB()]\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(\"accuracy: {:.2f}%\".format(acc * 100))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "accuracy: 57.80%\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "accuracy: 56.80%\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "accuracy: 59.80%\n"
     ]
    }
   ],
   "source": [
    "#Avalia a combinação dos dois primeiros descritores!\n",
    "\n",
    "#ATENÇÃO: ESTE É APENAS UM CÓDIGO EXEMPLO. VOCÊ DEVE DESENVOLVER\n",
    "#DESCRITORES MAIS ROBUSTOS, BEM COMO EXPLORAR MELHOR AS MÉTRICAS\n",
    "#DE AVALIAÇÃO (MATRIZ DE CONFUSÃO, ETC)\n",
    "\n",
    "trainAux = np.hstack((descHist, rawImages))\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(\n",
    "    trainAux, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(17),\n",
    "    DecisionTreeClassifier(),\n",
    "    GaussianNB()]\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(\"accuracy: {:.2f}%\".format(acc * 100))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "\n",
    "# Leitura da base de dados para as redes neurais:\n",
    "\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(\n",
    "    train, labels_n, test_size=0.25, random_state=42)\n",
    "\n",
    "#X_train,Y_train = read_dataset (TRAIN_DIR)\n",
    "X_train = np.asarray(X_train).reshape(-1, n_input)\n",
    "Y_train = np.asarray(Y_train)\n",
    "\n",
    "#X_test,Y_test = read_dataset (TEST_DIR)\n",
    "X_test = np.asarray(X_test).reshape(-1, n_input)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "#terceira forma de normalização\n",
    "#ATENÇÃO: para testar, comente todas as formas de normalização da função read_dataset()\n",
    "#scaler = StandardScaler()  \n",
    "#scaler.fit(X_train)  \n",
    "#X_train = scaler.transform(X_train)  \n",
    "#X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Variáveis do tensorflow:\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "X = tf.placeholder(tf.float32, [None, image_size * image_size * nchannels])\n",
    "prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\n",
    "# ESCOLHER DESCRITOR DE REDE\n",
    "#Ypred = mlp (X) \n",
    "Ypred = cnn (X, kprob)\n",
    "\n",
    "# Funções de custo:\n",
    "error1 = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(tf.nn.softmax(Ypred)), reduction_indices = [1]))\n",
    "error2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Ypred, labels = Y)) \n",
    "error3 = tf.reduce_mean(tf.reduce_sum(tf.square(tf.nn.softmax(Ypred) - Y), reduction_indices = [1]))\n",
    "error = error2\n",
    "    \n",
    "# Funções para minimização de erro: \n",
    "optimizer1 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(error)\n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(error)\n",
    "optimizer = optimizer2\n",
    " \n",
    "corr = tf.equal(tf.argmax(Ypred,1),tf.argmax(Y,1))\n",
    " \n",
    "accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: ', ' 1')\n",
      "  training loss:\t\t0.848206\n",
      "  validation accuracy:\t\t52.92 %\n",
      "('Epoch: ', ' 2')\n",
      "  training loss:\t\t0.690303\n",
      "  validation accuracy:\t\t55.98 %\n",
      "('Epoch: ', ' 3')\n",
      "  training loss:\t\t0.689215\n",
      "  validation accuracy:\t\t55.16 %\n",
      "('Epoch: ', ' 4')\n",
      "  training loss:\t\t0.643687\n",
      "  validation accuracy:\t\t61.35 %\n",
      "('Epoch: ', ' 5')\n",
      "  training loss:\t\t0.612530\n",
      "  validation accuracy:\t\t64.81 %\n",
      "('Epoch: ', ' 6')\n",
      "  training loss:\t\t0.578339\n",
      "  validation accuracy:\t\t68.75 %\n",
      "('Epoch: ', ' 7')\n",
      "  training loss:\t\t0.518985\n",
      "  validation accuracy:\t\t74.52 %\n",
      "('Epoch: ', ' 8')\n",
      "  training loss:\t\t0.455114\n",
      "  validation accuracy:\t\t79.14 %\n",
      "('Epoch: ', ' 9')\n",
      "  training loss:\t\t0.414634\n",
      "  validation accuracy:\t\t80.50 %\n",
      "('Epoch: ', '10')\n",
      "  training loss:\t\t0.384577\n",
      "  validation accuracy:\t\t82.81 %\n",
      "Final results:\n",
      "  test loss:\t\t\t0.653212\n",
      "  test accuracy:\t\t63.96 %\n"
     ]
    }
   ],
   "source": [
    "# Treina e Avalia os descritores e classidicadores de Redes\n",
    "\n",
    "\n",
    "# Inicialização de variáveis:\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "        \n",
    "    # Treino:\n",
    "    for epoch in range(nepochs):\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        total_batch = int(len(X_train)/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = next_batch (batch_size, X_train, Y_train)\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            err, acc = sess.run([error,accuracy], feed_dict={X: batch_xs, Y: batch_ys, prob: kprob})\n",
    "            train_err += err\n",
    "            train_acc += acc\n",
    "            train_batches += 1\n",
    "        print(\"Epoch: \", '%2d' % (epoch+1))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err/train_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(train_acc/train_batches * 100))\n",
    "        \n",
    "    # Testes:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    total_batch = int(len(X_test)/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = next_batch (batch_size, X_test, Y_test)\n",
    "        sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        err, acc = sess.run([error,accuracy], feed_dict={X: batch_xs, Y: batch_ys, prob: kprob})\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err/test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format((test_acc/test_batches)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
