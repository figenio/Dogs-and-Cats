{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ass Mateus e Yan\n",
    "\n",
    "# IMPORTAÇÕES\n",
    "import numpy as np\n",
    "import os, cv2, random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIÇÕES\n",
    "\n",
    "\n",
    "#TRAIN_DIR = 'kaggle/train/'\n",
    "TRAIN_DIR = 'cifar10/cifar_1/'\n",
    "\n",
    "num_img = 500                      # Opção do número de imagens a serem usadas\n",
    "n_classes = 10                     # A base de dados DogsAndCats tem 2 classes de objetos!!!!\n",
    "nepochs = 10                      # Numero de epocas para o treinamento!!!\n",
    "batch_size = 50                   # Numero de imagens por batch!!!\n",
    "image_size = 128                   # Todas as imagens devem ser redimensionadas para 32x32 pixels!!!\n",
    "nchannels = 3                     # Numero de canais de cores na imagem, tambem tem que mudar parte do codigo\n",
    "n_input = image_size * image_size * nchannels # Tamanho da entrada!\n",
    "learning_rate = 1e-3              # Taxa de aprendizado!!!\n",
    "kprob = 0.5                       # Probabilidade para dropout!!!\n",
    "test_size = 0.25                  # Porcentagem que sobra pra teste\n",
    "random_state = 42                 # Semente de aleatoriedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÕES AUXILIARES\n",
    "\n",
    "\n",
    "# Retorna a imagem apontada pelo endereço do arquivo redimensionada para o tamnho correto\n",
    "# Temos a opção de ler a imagem colorida ou em escala de cinza\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "#    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return cv2.resize(img, (image_size, image_size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "# Preparação dos Dados\n",
    "# Normaliza e passa as imagens no formato de um vetor de dados de tipo dtype\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, nchannels, image_size, image_size), dtype = np.float32)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        \n",
    "        #primeira forma de normalização\n",
    "        #min_val = np.min(image)\n",
    "        #max_val = np.max(image )\n",
    "        #image = (image-min_val)/(max_val-min_val)\n",
    "        #segunda forma de normalização\n",
    "        #image = image/255.0    \n",
    "        \n",
    "        data[i] = image.T\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))    \n",
    "    return data\n",
    "\n",
    "        \n",
    "#---------------------------------------------\n",
    "#Funções prontas das lista\n",
    "def create_label(image_name):\n",
    "    word_label = image_name.split('.')[-3]\n",
    "    if word_label == 'cat':\n",
    "        return np.array([1,0])\n",
    "    elif word_label == 'dog':\n",
    "        return np.array([0,1])\n",
    "    else: \n",
    "        print (\"Esta classe não existe!!!!!\")\n",
    "\n",
    "def read_dataset (filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for img in tqdm(os.listdir(filename)):\n",
    "        path = os.path.join(filename, img)\n",
    "        img_data = cv2.imread(path)\n",
    "        #img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE) #converte para níveis de cinza\n",
    "        img_data = cv2.resize(img_data, (image_size, image_size)) #deixa com as dimensões definidas\n",
    "        \n",
    "        #primeira forma de normalização\n",
    "        #min_val = np.min(img_data)\n",
    "        #max_val = np.max(img_data )\n",
    "        #img_data = (img_data-min_val)/(max_val-min_val)\n",
    "        #segunda forma de normalização\n",
    "        #img_data = img_data/255.0    \n",
    "        \n",
    "        #cria os vetores de dados e de labels\n",
    "        X.append(np.array(img_data))\n",
    "        Y.append(np.array(create_label(img)))\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "def next_batch (num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = labels[idx]\n",
    "    return data_shuffle, labels_shuffle\n",
    "\n",
    "\n",
    "# Printa a imagem de um gato e de um cachorro no indice apontado\n",
    "def show_cats_and_dogs(idx):\n",
    "    cat = read_image(train_cats[idx])\n",
    "    dog = read_image(train_dogs[idx])\n",
    "    pair = np.concatenate((cat, dog), axis=1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(pair)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOSSOS DESCRITORES\n",
    "\n",
    "#CNN igual ao do slide\n",
    "def cnn (x, prob):\n",
    "    \n",
    "    input_layer = tf.reshape(x, shape = [-1, image_size, image_size, nchannels])\n",
    "    \n",
    "    # Primeira camada de convolucao:\n",
    "    conv1 = tf.layers.conv2d (\n",
    "                inputs=input_layer,\n",
    "                filters=32,\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu\n",
    "            )\n",
    "    \n",
    "    # Primeira camada de pooling:\n",
    "    pool1 = tf.layers.max_pooling2d (\n",
    "                inputs=conv1, \n",
    "                pool_size=[2, 2], \n",
    "                strides=2\n",
    "            )\n",
    "    \n",
    "    # Segunda camada de convolucao:\n",
    "    conv2 = tf.layers.conv2d (\n",
    "                inputs=pool1,\n",
    "                filters=64,\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu\n",
    "            )\n",
    "    \n",
    "    # Segunda camada de pooling:\n",
    "    pool2 = tf.layers.max_pooling2d (\n",
    "                inputs=conv2, \n",
    "                pool_size=[2, 2], \n",
    "                strides=2\n",
    "            )\n",
    "    \n",
    "    flat = tf.contrib.layers.flatten(pool2)\n",
    "    \n",
    "    dense = tf.contrib.layers.fully_connected(\n",
    "                inputs = flat,\n",
    "                num_outputs = 128,\n",
    "                activation_fn = tf.nn.relu\n",
    "            )\n",
    "    \n",
    "    dropout = tf.nn.dropout(dense, prob)\n",
    "    \n",
    "    out = tf.contrib.layers.fully_connected(\n",
    "                inputs = dropout,\n",
    "                num_outputs = n_classes,\n",
    "                activation_fn = None\n",
    "            )\n",
    "    \n",
    "# Aqui podemos retornar tanto o descrito quanto o classificador\n",
    "    #return flat\n",
    "    return out\n",
    "\n",
    "\n",
    "# MLP da lista de exercícios\n",
    "def mlp (x):\n",
    "    # Aqui é definido o número de neurônios em cada camada\n",
    "    n_camada_1 = 2048\n",
    "    n_camada_2 = 512\n",
    "   \n",
    "    # Primeira camada da rede:\n",
    "    W1 = tf.get_variable('w1', [n_input, n_camada_1], initializer = tf.random_normal_initializer())\n",
    "    b1 = tf.get_variable('b1', [n_camada_1], initializer = tf.random_normal_initializer())\n",
    "    y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1) #tf.nn.relu() or tf.matmul(x, W1) + b1!!!! \n",
    "\n",
    "    # Segunda camada da rede:\n",
    "    W2 = tf.get_variable('w2', [n_camada_1, n_camada_2], initializer = tf.random_normal_initializer())\n",
    "    b2 = tf.get_variable('b2', [n_camada_2], initializer = tf.random_normal_initializer())\n",
    "    y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2) \n",
    "\n",
    "    # Ultima camada da rede:\n",
    "    W3 = tf.get_variable('w3',[n_camada_2, n_classes], initializer = tf.random_normal_initializer())\n",
    "    b3 = tf.get_variable('b3',[n_classes], initializer = tf.random_normal_initializer())\n",
    "    out_layer = tf.matmul(y2, W3) + b3 \n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 5000\n",
      "Processed 250 of 5000\n",
      "Processed 500 of 5000\n",
      "Processed 750 of 5000\n",
      "Processed 1000 of 5000\n",
      "Processed 1250 of 5000\n",
      "Processed 1500 of 5000\n",
      "Processed 1750 of 5000\n",
      "Processed 2000 of 5000\n",
      "Processed 2250 of 5000\n",
      "Processed 2500 of 5000\n",
      "Processed 2750 of 5000\n",
      "Processed 3000 of 5000\n",
      "Processed 3250 of 5000\n",
      "Processed 3500 of 5000\n",
      "Processed 3750 of 5000\n",
      "Processed 4000 of 5000\n",
      "Processed 4250 of 5000\n",
      "Processed 4500 of 5000\n",
      "Processed 4750 of 5000\n",
      "Train shape: (5000, 3, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "#MONTANDO A BASE DE TREINAMENTO\n",
    "\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] #full dataset\n",
    "#train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "#train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "train_airplane =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'airplane' in i]\n",
    "train_automobile =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'automobile' in i]\n",
    "train_bird =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'bird' in i]\n",
    "train_cat =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "train_deer =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'deer' in i]\n",
    "train_dog =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "train_frog =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'frog' in i]\n",
    "train_horse =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'horse' in i]\n",
    "train_ship =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'ship' in i]\n",
    "train_truck =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'truck' in i]\n",
    "# considera apenas num_img imagens. Para o dataset completo, desconsiderar.\n",
    "train_images = (train_airplane[:num_img]\n",
    "                + train_automobile[:num_img]\n",
    "                + train_bird[:num_img]\n",
    "                + train_cat[:num_img]\n",
    "                + train_deer[:num_img]\n",
    "                + train_dog[:num_img]\n",
    "                + train_frog[:num_img]\n",
    "                + train_horse[:num_img]\n",
    "                + train_ship[:num_img]\n",
    "                + train_truck[:num_img])\n",
    "                \n",
    "random.shuffle(train_images)\n",
    "\n",
    "# Leitura das imagens\n",
    "train = prep_data(train_images)\n",
    "print(\"Train shape: {}\".format(train.shape))\n",
    "\n",
    "# Cria os labels (rótulos)\n",
    "labels = []\n",
    "for i in train_images:\n",
    "    if 'airplane' in i:\n",
    "        labels.append(np.array([1,0,0,0,0,0,0,0,0,0]))\n",
    "    elif 'automobile' in i:\n",
    "        labels.append(np.array([0,1,0,0,0,0,0,0,0,0]))\n",
    "    elif 'bird' in i:\n",
    "        labels.append(np.array([0,0,1,0,0,0,0,0,0,0]))\n",
    "    elif 'cat' in i:\n",
    "        labels.append(np.array([0,0,0,1,0,0,0,0,0,0]))\n",
    "    elif 'deer' in i:\n",
    "        labels.append(np.array([0,0,0,0,1,0,0,0,0,0]))\n",
    "    elif 'dog' in i:\n",
    "        labels.append(np.array([0,0,0,0,0,1,0,0,0,0]))\n",
    "    elif 'frog' in i:\n",
    "        labels.append(np.array([0,0,0,0,0,0,1,0,0,0]))\n",
    "    elif 'horse' in i:\n",
    "        labels.append(np.array([0,0,0,0,0,0,0,1,0,0]))\n",
    "    elif 'ship' in i:\n",
    "        labels.append(np.array([0,0,0,0,0,0,0,0,1,0]))\n",
    "    elif 'truck' in i:\n",
    "        labels.append(np.array([0,0,0,0,0,0,0,0,0,1]))\n",
    "    else:\n",
    "        print('Sem classe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-a3ebc09565f4>:34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leitura da base de dados para as redes neurais:\n",
    "\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(\n",
    "    train, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "#X_train,Y_train = read_dataset (TRAIN_DIR)\n",
    "X_train = np.asarray(X_train).reshape(-1, n_input)\n",
    "Y_train = np.asarray(Y_train)\n",
    "\n",
    "#X_test,Y_test = read_dataset (TEST_DIR)\n",
    "X_test = np.asarray(X_test).reshape(-1, n_input)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "#terceira forma de normalização\n",
    "#ATENÇÃO: para testar, comente todas as formas de normalização da função read_dataset()\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Variáveis do tensorflow:\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "X = tf.placeholder(tf.float32, [None, image_size * image_size * nchannels])\n",
    "prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\n",
    "# ESCOLHER DESCRITOR DE REDE\n",
    "Ypred = mlp (X) \n",
    "#Ypred = cnn (X, kprob)\n",
    "\n",
    "# Funções de custo:\n",
    "error1 = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(tf.nn.softmax(Ypred)), reduction_indices = [1]))\n",
    "error2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Ypred, labels = Y)) \n",
    "error3 = tf.reduce_mean(tf.reduce_sum(tf.square(tf.nn.softmax(Ypred) - Y), reduction_indices = [1]))\n",
    "error = error2\n",
    "    \n",
    "# Funções para minimização de erro: \n",
    "optimizer1 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(error)\n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(error)\n",
    "optimizer = optimizer2\n",
    " \n",
    "corr = tf.equal(tf.argmax(Ypred,1),tf.argmax(Y,1))\n",
    " \n",
    "accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mateus/.local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "('Epoch: ', ' 1')\n",
      "  training loss:\t\t6.330737\n",
      "  validation accuracy:\t\t39.31 %\n",
      "('Epoch: ', ' 2')\n",
      "  training loss:\t\t3.867949\n",
      "  validation accuracy:\t\t46.64 %\n",
      "('Epoch: ', ' 3')\n",
      "  training loss:\t\t3.419136\n",
      "  validation accuracy:\t\t49.07 %\n",
      "('Epoch: ', ' 4')\n",
      "  training loss:\t\t3.077545\n",
      "  validation accuracy:\t\t52.40 %\n",
      "('Epoch: ', ' 5')\n",
      "  training loss:\t\t2.756689\n",
      "  validation accuracy:\t\t53.20 %\n",
      "('Epoch: ', ' 6')\n",
      "  training loss:\t\t2.535883\n",
      "  validation accuracy:\t\t55.25 %\n",
      "('Epoch: ', ' 7')\n",
      "  training loss:\t\t2.401914\n",
      "  validation accuracy:\t\t55.73 %\n",
      "('Epoch: ', ' 8')\n",
      "  training loss:\t\t2.198838\n",
      "  validation accuracy:\t\t58.13 %\n",
      "('Epoch: ', ' 9')\n",
      "  training loss:\t\t2.055178\n",
      "  validation accuracy:\t\t59.36 %\n",
      "('Epoch: ', '10')\n",
      "  training loss:\t\t2.102956\n",
      "  validation accuracy:\t\t57.01 %\n"
     ]
    }
   ],
   "source": [
    "# Treina e Avalia os descritores e classidicadores de Redes\n",
    "\n",
    "\n",
    "# Inicialização de variáveis:\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "        \n",
    "    # Treino:\n",
    "    for epoch in range(nepochs):\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        total_batch = int(len(X_train)/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = next_batch (batch_size, X_train, Y_train)\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            err, acc = sess.run([error,accuracy], feed_dict={X: batch_xs, Y: batch_ys, prob: kprob})\n",
    "            train_err += err\n",
    "            train_acc += acc\n",
    "            train_batches += 1\n",
    "        print(\"Epoch: \", '%2d' % (epoch+1))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err/train_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(train_acc/train_batches * 100))\n",
    "        \n",
    "    # Testes:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    total_batch = int(len(X_test)/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = next_batch (batch_size, X_test, Y_test)\n",
    "        sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        err, acc = sess.run([error,accuracy], feed_dict={X: batch_xs, Y: batch_ys, prob: kprob})\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err/test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format((test_acc/test_batches)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
